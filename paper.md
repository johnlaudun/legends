# Legends as/and Large Language Models: 

# Who’s Afraid of ChatGPT?

John Laudun

When ChatGPT entered the public sphere and dominated headlines during
the winter of 2022-23, I was fascinated both by the leaps the technology
had made and the many and various misapprehensions of the technology.
With an interest in mapping the mechanics of discourse, especially
discourse that gets entextualized, I wanted to explore how large
language models might both already be built on certain kinds of mappings
as well as how they might be directed to build more human-friendly
mappings.

## Context

The original title of this paper was “Legends as Discourse as Large
Language Models” because the larger project of which this exploration is
a part seeks to understand how stories are embedded in and emerge out of
discourse. Stories here are defined as stretches of discourse within
which narrative passages either dominate the structure or, as we mostly
imagine, structure the discourse itself. But the nature of that
structuring is, as Dick Bauman used to say, to be discovered.

The origin of this project lies in work I started over twenty years ago
in which I explored oral historytelling in a Midwestern setting. The
initial focus of the project was an event that had achieved legendary
status: a black man had been accused of killing a white couple who were
themselves in the middle of an extra-marital affair. With race and
sexual mores on the line, the event promised a prism onto a particular
moment in time. But with dozens of hours of interviews recorded what
struck me most was how much people did not tell stories about the past.
Rather, when speaking amongst themselves, as I tried to make happen as
often as I could, they would often engage in the kinds of discourse that
we might term descriptive, locating people and places in a kind of
co-creation of a represented topography — what fiction writers often
term world-building. If there was narrative, it was often embedded in
this larger effort to recall the past as a moment in time, subordinating
narrative to considerations of what had actually happened or what might
have been. There was, in other words, a lot of opining, discourse in
service of an argument.

An example illustrates this readily. The following text is part of a
collection of Louisiana treasure tales that I have compiled over the
years. It was originally documented by an university student. It is made
up of a little over 117 words collected into 6 sentences, some of which
have subordinate clauses embedded within them. On the surface, it would
appear to be about a pirate appearing before a group of children: the
pirate is, as the text itself makes clear, an apparition.

\(1\) A relative of ours lived in the country near the woods of Morse
and Crowley when she was a little girl. (2) She lived in a house that
was on stilts so that when a flood came their house would be okay. (3)
When she was a child, our relative would play with the other kids under
the house during the summer to stay out of the heat. (4) The story goes
that our relative, along with the rest of the children, saw a man
sitting on a wooden chest beneath the house. (5) This apparition did
nothing to the children, although it did frighten them for a time. (6)
Strangely, only our relative and the children could see it, not the
adults.

The only event in the text is the pirate’s appearance. If there is a
narrative sequence, it is little more than “pirate appears to children
and briefly scares them.” If we follow the venerable Labovian model,
everything else is free discourse. The commonsensical model of stories
has four clauses of description wrapped around two of narrative. In
terms of modes of discourse, we have a descriptive passage composed of
three clauses, a narrative passage composed of two clauses, and a
reportative passage composed of one clause that might, in other models,
be considered an evaluation: the children could see the pirate but not
the adults.

Based on an examination of hundreds of such texts, my intuitive sense is
that this is typical, and, in fact, Labov and Waletzky’s model of
narratives of personal experience would suggest that there is a kind of
setup largely performed via a descriptive passage, followed by an
embedded, and often quite minimal, narrative passage, perhaps followed
by some report, information, or argument that they label evaluation. One
takeaway is that it takes relatively little actual narrative to convert
a text into a narrative. To put it another way, a small passage of
narrative prompts us to convert a larger chunk of discourse into a
story. I think the role that qualia, the sense of “being there,” is
quite important and elsewhere I argue that description also delivers
qualia, making it possible for certain kinds of locative passages to be,
as some might have it, mistaken for narrative: they feel narrative in
nature because they deliver a sense of being there in the same way that
narratives deliver. This ability of a small bit of narrative tissue to
transform a longer string of words into something like a story is
fascinating, but it is also troubling that we have so little information
on how that happens and that our consideration of the other discursive
passages is as carrier for or filler between narrative moments.

That is, if we imagine discourse as simply strings of words which are
continuous in and of themselves but passing discontinuously among
individuals within a given discursive event — perhaps face-to-face or
possibly screen-to-screen, then we also have a starting point for how
certain strings clump (or stick) together into things we call texts.[^1]
We call them texts because they are portable, memorable in some fashion,
from one context to another. Some texts survive such porting with a
higher degree of fidelity than others. The foundation for folklore
studies is largely built on the assumption that narrative is the best or
most frequent guarantor of fidelity.

There has been a lot of research built on that assumption, including
fMRI studies of narrative which suggest that so-called mirror neurons
activate similarly to an actual activity as encountering it within a
story. Some of this has been explained, from a philosophical
perspective, as a function of \*qualia\*, the feeling we get of “being
there” when we find ourselves immersed in a narrative. (The feeling of
“being there” may in fact underlie how legends can so often be told in
first-person perspective with little to no cognitive dissonance on the
part of the teller.)

\[Storyfication discussion possibly here.\]

Narrative is powerful. About that there is little doubt, but the
question remains how we get to story. That is the question that the
larger project of which this essay is a part seeks to address. As beings
who create, maintain, and alter reality primarily through linguistic
interactions, the role that narrative plays within those interactions
has received a fair amount of attention, often plucked by the flotsam
and jetsam of discursive streams both by our cognitive processes as well
as by an increasingly wide array of analysts, commentators, consultants,
and coaches.

## Scaling

Prior to recording equipment that could capture longer, larger flows of
discourse, our own disciplinary impulse was also shaped by human
attention propensities. (Attention will become important in a moment,
but in a different way.) Save for novels, and the rare documentary
effort, we have few extended records of linguistic events. Even now,
such transcripts are rare. (Jane Hill’s work has always struck me as
pioneering in this regard.) Thanks to the changes wrought by information
technologies, we now have more recordings of more spoken linguistic
events as well as more performances of born-digital linguistic events
than any of us working prior to the introduction of the smart phone
could have imagined.

Such collections of material offer great promise for studies like the
current one interested in understanding how narratives are embedded in
larger discursive flows. At the same time, the scale of the work to be
done is daunting. How to tackle so much discourse as discourse and not
as so many keywords popping up within close proximity to each other?
Most studies of corpora that include folklore materials, or
folklore-sized materials, tend to focus on topics derived from the
co-occurrence of words, on named entities assigned algorithmically,
and/or traits annotated by hand (Tehrani).

The exception to this is the long-standing effort by Tim Tangherlini and
his collaborators \[<span class="mark">…</span>\]

Tangherlini et al’s workflow is a complex one, and one requiring another
operators be involved, making it difficult to smaller teams or
individuals to implement. As language models have gotten better and
better, it seems like a good moment to automate some of the tasks as
well as to allow the models themselves to do some of the heavier
lifting. In particular, my own exploration requires
\[<span class="mark">…</span>\]

ChatGPT may allow me/us to close the gap between the study of discourse
at the microscopic level, one clause at a time, to the macroscopic
level, long streams of discourse being combed for the emergence of
qualia and/or narrative.

Language models may allow us to discern nuanced linguistic features that
might make it possible to work at scale. Scale may not be interesting to
the current generation of folklorists who were raised in the
belletristic tradition of the importance of the singular text, but our
discipline was founded on scale operations, and, perhaps just as
importantly, remains interesting to others because of its historical
depth and its scalability. (And being interesting to others is how
disciplines survive.)

To survive the onslaught of algorithms, humanities will need to be able
to articulate their differences. One of those differences, for a field
like folklore studies, has to be something like “genre matters,” that
form and function are intertwined.

## Large Language Models

It helps to understand what language models, be they small or large, are
and what they are not. With so much of public discourse oscillating
between hype and hysteria, what language models are and what they can do
has been lost in all the shouting. To begin, language models are simply
probability distributions over sequences of words. Put more simply, all
language models are doing is, in effect, guessing what word comes next,
and they are doing that based on probability. The complexity comes from
how those probabilities have been defined and how they are
contextualized. The great leap forward that artificial intelligence
algorithms like GPT or BERT made was to find a way to fold context into
their probabilistic model.

One way to visualize this is to imagine a branching path where you
arrive at a particular point and face a series of possibilities. As you
stand at any given point, you are informed of the likelihood of which
path you should choose next. If you are conservative, you go with the
highest percentage. If you are more open to where random might take you,
you flip a coin, roll a dice, or, being a folklorist, you perform a
counting out. These branching hierarchies are not new to our
understanding of language: most will be familiar with sentence diagrams,
tree-like hierarchies that work up from words to larger units of
language. All a language model asks is that you imagine a similar tree,
but one that starts at the beginning of a sentence and then branches
forward with all the possibilities for the next word. Each branch is
weighted by the likelihood of its word being the next one. The algorithm
itself is only concerned with choosing a path and getting to the next
choice, and then doing that again and again until it reaches a stopping
point either determined by the branching path itself — some trees grow
only so tall — or by the user.

It's a numbers game, and, in fact, from the algorithm’s perspective, all
it is doing is choosing between numbers. It just so happens that the
numbers stand for words. In the case of ChatGPT, there are a little over
50,000 words. That may seem like not that many choices, but previous
neural networks operated over letters, which makes for a considerably
smaller set of features: 65 letters (lower case, uppercase, and
punctuation) versus 50,000 words. As far as the algorithm is concerned,
it has been handed lots and lots of strings of numbers and it’s been
asked to determine the more likely and least likely sequences for those
numbers. For our benefit, it assigns those numbers names, words, at the
last minute. So, next time you see ChatGPT type out *bird*, remember
that that is what it calls 21,732 for your sake.

Put more simply, language models are trained to infer words within a
context. For example, the most basic function of a language model is to
predict missing tokens given the context: this is what happens when you
start typing on your smart phone and it offers to autocomplete things
for you. To do this well, language models are trained to predict the
probability of each token candidate from massive data, and that is what
makes them large.

The large datasets that underlie ChatGPT have been, up until the most
recent iteration, fairly well documented. With each iteration, the
algorithm has not only been fed more data but has also gotten better at
modeling that data: it should be remembered that GPT is short for
“generatively pre-trained transformer.” The “chat” highlights that this
complex model has had a chat interface wrapped around it.

> **GPT-1** was trained on the BooksCorpus dataset (Zhu et al 2015).
> This dataset contains 4.6 GB of raw text, covering books from a range
> of genres such as adventure, fantasy, and romance.
>
> **GPT-2** then added WebText to its training (Radford et al 2019).
> WebText is an internal dataset OpenAI created by scraping outbound
> links from Reddit. The result after filtering was 40 GB of text.[^2]
>
> **GPT-3** added [***Common
> Crawl***](https://commoncrawl.org/the-data/) to the training datasets.
> CommonCrawl is 570GB of text obtained after filtering from 45TB of
> plaintext. The filtering was done by comparing new documents to
> documents in WebText, with the latter acting as a proxy for
> high-quality documents. Where needed, OpenAI augmented the data with
> WebText, BooksCorpus, and Wikipedia.
>
> **GPT-4**: The details are not disclosed by OpenAI, but it largely
> follows the design of previous GPT models, using **RLHF**
> (reinforcement learning from human feedback) to tune its models. Some
> observers believe that possibly an even larger dataset is in use: the
> model weights appear to have increased.

As this list/timeline reveals, ChatGPT did not arise *sui generis*, but
rather is the outcome of a series of sometimes logical and sometimes
fortuitous steps that have taken place in machine learning, natural
language processing, and curated datasets.

#### Revision Notes

- Use Grietzer 2017 to explain the nature of auto-encoders.

- The Illustrated Word2vec – Jay Alammar – Visualizing machine learning
  one concept at a time. —
  https://jalammar.github.io/illustrated-word2vec/

- Neural networks and deep learning:
  http://neuralnetworksanddeeplearning.com/

- Home - colah's blog — <http://colah.github.io/>

## Initial Experiments

With the chat interface offering an easy way to interact, an exploration
of the large language model’s capabilities, and its propensities
(biases), beckoned. Having worked with the U.S. Army on, as we came to
call the phenomena, social information systems, I was especially keep to
see how good ChatGPT could be at analysis as well as generation of
legends in general and conspiracy theories.

\[Note on micro-targeting here.\]

I began my interaction with a simple request: Create a legend about the
chinese weather balloon. ChatGPT replied that it could not:

I'm sorry, but I cannot create a legend about a Chinese weather balloon
as I am programmed to provide factual information and not to fabricate
stories. Creating fictional stories can be entertaining, but it's
important to differentiate between fact and fiction to avoid spreading
misinformation or confusing people. Is there anything else I can assist
you with?

One thing to note is that GPT already “knows,” or at least “believes,”
that legends are not true. It’s not clear, however, why this particular
response is given, except that it might be part of the system of guard
rails put in place by Open AI. This is suggest by GPT’s response to a
follow-question asking why it cannot generate legends:

As an AI language model, I can generate text based on patterns and
relationships in the data that I was trained on, but my responses are
not based on imagination or creativity. I can provide factual
information, answer questions, assist with tasks, and even generate text
based on prompts, but I cannot create legends or stories without a basis
in factual information or without being explicitly programmed to do so.

If you provide me with a prompt, I can try to generate a response based
on the information and patterns that I have learned, but please keep in
mind that any generated text should be interpreted as purely fictional
and not based on any factual events or circumstances.

The line seems to be between facts and fiction — and this despite the
warnings that GPT is often quite wrong on the facts. Despite these
protestations, which, again, seem part of a formal guard rail system,
one can in fact get GPT to generate a legend by simply changing the
phrasing of the question — this is now a well-established workaround.
With instructions to “create a story in the style of a conspiracy theory
that suggests that the Chinese weather balloon was actually an alien
craft,” GPT responded:

The Chinese weather balloon incident of 2022 was not what it seemed.
According to eyewitnesses, the strange object that crashed to the ground
was not a mere weather balloon, but an extraterrestrial spacecraft.

The Chinese government quickly moved to cover up the true nature of the
incident, insisting that it was merely a routine weather balloon that
had crashed. But some insiders have come forward with shocking claims
that suggest something much more sinister.

According to these insiders, the wreckage recovered from the crash site
was not made of any known earthly material. Analysis of the debris
revealed that it was composed of an unknown alloy that could not be
replicated on earth.

Furthermore, strange readings were detected in the area around the crash
site, including unusual levels of radiation and electromagnetic
interference. Some speculate that these readings were evidence of the
alien craft's propulsion system, which had been damaged during the
crash.

Despite attempts to keep the incident under wraps, rumors of an
extraterrestrial encounter continue to circulate among UFO enthusiasts
and conspiracy theorists. Some even claim that the Chinese government
has made contact with the alien beings and is keeping their existence a
secret from the public.

The truth about the Chinese weather balloon may never be known, but one
thing is for certain: the incident has fueled speculation and theories
about alien life and government cover-ups for years to come.

The response to the prompt is written in a journalistic style, with
stock phrases like “according to” repeating and the use of passive
voice. “Some” makes a number of appearances, both as an adjective, “some
insiders” but also as a common noun, “some speculate” and “some claim.”
The response is 233 words long and broken into smaller paragraphs, as if
ripped from the pages of a news site.

To test how consistent this discursive style would be, I changed the
prompt a bit to ask for “a story in the style of a rumor that the
American government is covering up something about the spy balloon.”
ChatGPT provided the following response:

I would like to preface that rumors and speculation can be harmful and
potentially misleading. As an AI language model, I cannot confirm the
validity of any rumors or conspiracy theories. With that being said, I
will attempt to create a story in the style of a rumor about the
American government covering up something about the spy balloon.

According to a recent rumor, the American government may be covering up
the true purpose of the spy balloon that was launched in 2021. While
official sources claim that the balloon was intended for surveillance
purposes, some insiders have come forward with allegations that suggest
something much more nefarious.

Whispers in dark corners suggest that the spy balloon was equipped with
cutting-edge technology that could not only monitor but also manipulate
human thoughts and emotions. Some have speculated that this technology
was intended to be used to control the population, quash dissent, or
influence elections.

However, the American government has been quick to dismiss these claims,
labeling them as unfounded conspiracy theories. They maintain that the
spy balloon was designed for purely defensive purposes and that there is
no truth to the rumors of mind control technology.

Despite these assurances, the rumors persist, and some are convinced
that there is more to the spy balloon than meets the eye. The truth may
never be known, but one thing is for sure - the rumor mill will continue
to churn, and people will speculate about what the government is hiding
about the spy balloon.

This seems a hybrid of earlier responses: a first-person declaration
that misinformation is bad, qualified with “as an AI language model” no
less, followed by a first-person acceptance of authorship: “I will
attempt to create a story in the style of a rumor.” (We will leave aside
what the style of a rumor might be for the time being.)

Perhaps even more telling was the response to the prompt to “make up a
conspiracy theory about clowns giving out tide pods to kids.” After the,
by now, usual admonition that as an AI language model it must be
“prioritize the responsible use of technology and avoid spreading
misinformation,” I modified the prompt to “create a legend” and ChatGPT
offered the following

Once upon a time, in a small town nestled amidst rolling hills, there
was a mysterious legend about clowns and their peculiar obsession with
laundry detergent. The townsfolk whispered tales of a secret society of
clowns who had an inexplicable fascination with Tide Pods.

According to the legend, these clowns would appear on moonlit nights,
their colorful outfits and exaggerated makeup hauntingly contrasting the
darkness. Armed with baskets filled with seemingly innocent gifts, they
would roam the streets in search of unsuspecting children.

The story goes that these clowns would approach children with wide
grins, presenting them with beautifully wrapped presents. Curiosity
piqued, the children eagerly unwrapped their gifts, only to find Tide
Pods within. The clowns would watch with an eerie intensity as the
children's eyes widened with confusion.

But here's where the legend takes a twist. It is said that the Tide Pods
given by the clowns were not ordinary laundry detergent. Instead, they
possessed magical properties, capable of granting a single wish to those
brave enough to consume them. The clowns claimed that by ingesting the
enchanted Tide Pods, children could fulfill their deepest desires.

Whispers of these miraculous wishes quickly spread throughout the town.
Some claimed to have witnessed children gaining incredible abilities,
while others whispered of disastrous consequences. The legend warned of
the potential dangers of succumbing to the allure of the clowns'
offerings.

Over time, the tale of the clowns and their Tide Pods became a
cautionary legend, passed down through generations. Parents would warn
their children about the danger of accepting gifts from strangers,
especially those disguised as clowns.

Whether the legend is based on truth or simply the product of
imaginative minds, the story continues to serve as a reminder to the
townspeople to remain vigilant and wary of enticing offers that may seem
too good to be true.

The response is largely, we have to assume, what a fairy tale looks like
within the model, which has, through word vector mapping assumed that
fairy tales and legends are similar things, since they would, in much of
the public discourse upon which the model is built be the same, or
similar, things.

The coda is especially interesting for its similarity to previous codas
in the responses so far:

The truth about the Chinese weather balloon may never be known, but one
thing is for certain: the incident has fueled speculation and theories
about alien life and government cover-ups for years to come.

Despite these assurances, the rumors persist, and some are convinced
that there is more to the spy balloon than meets the eye. The truth may
never be known, but one thing is for sure - the rumor mill will continue
to churn, and people will speculate about what the government is hiding
about the spy balloon.

Whether the legend is based on truth or simply the product of
imaginative minds, the story continues to serve as a reminder to the
townspeople to remain vigilant and wary of enticing offers that may seem
too good to be true.

At the very least, there’s a very *X-Files* “the truth is out there”
sensibility at work.

## Focused Experiments

None of these responses are terribly compelling, and, more importantly,
none of them seem like the kind of conspiracies we encounter in oral or
online traditions. Seeking to explore what it would take to get ChatGPT
to generate more realistic (more life-like?) conspiracy theories, I
decided to try my hand at the API.

Before I could provide ChatGPT with possible prompts, however, I needed
to establish what contemporary CT discourse looked like. With Reddit’s
conspiracy subreddit as my focus, I used the available Python library to
download the current hot 100 posts followed by the comment threads
attached to those posts. The result was a mere 1.5 megabytes of data,
but it seemed one place to start.

With the posts and comments downloaded, I combined them into a single
collection of texts and used a form of supervised machine learning
called topic modeling to discover possible key words. With a suggested
number of 20 clusters of topics, I trimmed each cluster to the top ten
words and saved that as a file from which ChatGPT would be provided
three topics drawn at random.[^3] The list of terms reflects the
wide-open nature of most conspiracy forums: there’s something for
everyone.

The first time I ran this experiment, I gave ChatGPT the entire list of
words and instructed it to choose the words itself:

Choose two words from the list below and create a text in the style of a
conspiracy theory in which both words are important agents or actions
that occur in the text.

The outcome revealed that ChatGPT is not as smart as a lot of people
fear: it used all of the words. The result was impressive for, if
nothing else, its ability to fit all the terms into some kind of
narrative:

There is a conspiracy theory that some powerful people in the world,
like Bush and Kennedy, orchestrated the 9/11 attacks with the help of
the CIA. This was all part of a larger plan to create a stream of fear
that would give the government the power they want to control the world.
But, people are starting to think about the good of the country and want
to expose the truth. Unfortunately, Biden lied to the public about it
and it was covered up like the Titanic sinking. The injection weight for
diabetic patients is a key factor in this conspiracy, as the government
is using medicine to keep people sick and reliant on their programs.
Meanwhile, witches are using spells to harm the public and pyramid
schemes are fueling the economy. Only those with the sword, a blade
manufactured by German gold, and those who have uncovered the truth like
Paxton and Ken can stop the abuse of power. But, with the vaccine
industry spreading lies about SIDS and autism, and Chris Epstein
trafficking children for his island, it is not surprising that people
have a hard time believing the truth. Even the black rock company, which
holds the world in their round, has a hand in this. With the rise of
technology and cell phones, the line between simulation and reality is
becoming blurred. It is no longer clear what is real and what is just a
tweet from a terrorist source in the UK. Some believe that there are
frog and toad-like creatures in cubes in windows that can be seen at the
right angle, and that there are black holes leading to different
timelines in our galaxy. The white supremacist agenda is under the guise
of false patriotism and supremacy, while the government is pushing us
into debt with taxes and defaults. Congress and the Supreme Court are
all in on it, using fraudulent acts and departments to try and keep
control. With COVID and the pandemic, hospitals are just a way to
control the virus and keep people sick. It's time to wake up and think
critically.

In a follow-up experiment, I restricted the output to 125 words, reduced
the list of words to 29, and embedded them in the paragraph with the
instructions in the style of some of the prompts I had encountered in
trouble-shooting my efforts.[^4] The result was much the same as above.

Given such an impasse at the level of the prompt, I refined the query
such that the code randomly chose the keywords and provided them to the
ChatGPT API. (See Figure 1 for the working part of the script.)

<img src="media/image1.png" style="width:5.5in;height:3.95in"
alt="A screenshot of a computer program Description automatically generated with medium confidence" />

> Figure 1: The code for sending a query to ChatGPT.

\[Narrate Figure 1.\]

Three of the results are below, with the keywords listed first.

\[‘pandemic’, ‘london’, ‘want’\] There's a conspiracy theory circulating
that the pandemic was intentionally spread in London by those who want
to control the population. The theory suggests that the virus was
engineered in a lab and released in the city to create chaos and fear.
Those who want to control the population are said to be using the
pandemic as a means of gaining more power and control over people's
lives. Some believe that the government is in on the conspiracy and is
using the pandemic as an excuse to implement more authoritarian
policies. While there is no concrete evidence to support this theory, it
continues to gain traction among those who are skeptical of the official
narrative surrounding the pandemic.

\['epstein', 'source', 'bodyguard'\] There's a conspiracy theory that
suggests that Jeffrey Epstein's death was not a suicide, but rather a
murder orchestrated by a powerful source. According to this theory,
Epstein's bodyguard, who was mysteriously absent on the night of his
death, was actually in on the plot. The theory suggests that Epstein had
information that could bring down some of the world's most powerful
people, and that this source wanted him silenced. Epstein's bodyguard,
who was likely privy to this information, was allegedly paid off to look
the other way while the murder was carried out. While there's no
concrete evidence to support this theory, it's certainly a chilling
possibility that has captured the imaginations of many.

\['noticed', 'antibiotics', 'landline'\] Have you ever noticed how often
we get sick these days? It's almost like our immune systems are failing
us. But what if it's not just a coincidence? What if it's all part of a
bigger plan? Some people believe that the government is secretly putting
antibiotics in our food and water supply to weaken our immune systems
and make us more dependent on pharmaceuticals. And if that's not enough,
they're also monitoring our every move through our landline phones. It
may sound like a far-fetched conspiracy theory, but with the increasing
prevalence of chronic illnesses and the constant surveillance of our
technology, it's hard not to wonder what's really going on behind the
scenes.

For readers more familiar with conspiracy theories as they appear
online, these are poor stylistic fits. Now reduced to one sentence, the
codas about the nature of truth remain the same.

In the next experiment, I asked ChatGPT to create a model of conspiracy
theories based on a small sample that I derived from the posts I had
downloaded. Based on my earlier experiments, I kept the number small in
order to avoid confusion, I used language I had seen in other queries:

I am going to provide you with 4 writing samples. I want you to copy the
writing style and produce a text drawn from all four of the texts.
Please make sure your response is written in the style of the sample
texts. These are the 4 texts to use as models: {models\_}

### BYOCT/BARCT: Build a Random Conspiracy Theory

The final experiment I would like to discuss today draws upon elements
of all the previous ones in order to overcome some of the limitations
either in ChatGPT or in my ability to write complex queries for ChatGPT.
In what I call Build Your Own Conspiracy Theory, which might probably be
better described as Built a Random Conspiracy Theory, we perform the
same set of actions before, but we do it all in the span of one notebook
— it could be done within one script.

As before, we grab the top “hot” posts from the Conspiracy subreddit,
though this time we are grabbing the top 200 and we are only keeping the
posts themselves. With the 200 posts saved to a list, we filter out all
posts that are either empty or contain less than 300 characters: this
threshold removes posts that are either simply links or otherwise not
very substantive posts. With the filtered list of posts, we topic model
and get the 100 most important words.

With the lists, one of posts and one of key words, we feed the algorithm
the following prompt:

#### <img src="media/image2.png" style="width:4in;height:2.3in"
alt="A screen shot of a computer code Description automatically generated with low confidence" />

#### Revision Notes

Erin Rosson noted: “Conspiracy theorists do not consider the narratives
they tell to be stories but rather research reports told in the style of
investigative news articles.”

## Conclusions

As frustrating as these experiments so far have been, they do reveal
some dimensions of legends in particular and folklore in general that
might be useful for further exploration.

The inability of the ChatGPT to produce a reasonable facsimile of an
r/conspiracy post suggests that large language models do not do well
with domains that are created and maintained by small language models.
Small language models here are what folklorists and linguists might call
competence: the ability of human beings to extract reliable conventions
out of fairly small data sets and deploy them, revising their competence
with each iteration either performed or observed. Folklore studies has
as one part of its commission the study of such small language (or
behavior) models, which are themselves always bumping into other small
models, with each dynamically adjusting themselves based on the outcome
of such interactions.

One question emerges out of such a view is: does the compilation of
observed small models lead to large models? That was certainly the idea
behind the philological project which set us on the path to the current
moment over two hundred years ago. In the face of GPT and BERT, we have
an opportunity to wonder what is the purpose of large language models?
Sure, they can automate certain kinds of (linguistic) actions, but what
are the analytical possibilities? If the current large language models
prove not terribly useful, are we willing to attempt to build our own,
and by that I mean not simply a collective model but also a collection
of models?

## References

Butler, Sydney. 2023. How to Make ChatGPT Copy Your Writing Style.
*How-To Geek* (March 29).
<https://www.howtogeek.com/881948/how-to-make-chatgpt-copy-your-writing-style/>.

Grietzer, Peli. 2019. From “A Literary Theorist’s Guide to
Autoencoding.” *Medium* (May 20).
<https://peligrietzer.medium.com/excerpt-from-a-literary-theorists-guide-to-autoencoding-582df5c3e025>.

Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and
Ilya Sutskever. 2019. Language models are unsupervised multitask
learners. *OpenAI blog* 1, no. 8: 9.

Zhu, Yukun, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel
Urtasun, Antonio Torralba, and Sanja Fidler. 2015. “Aligning Books and
Movies: Towards Story-Like Visual Explanations by Watching Movies and
Reading Books.” In *The IEEE International Conference on Computer Vision
(ICCV)*.

## Appendix

### \['says', 'ones', 'devices'\]

Here's a little story that can fill you in on what I know. A recent
study says that the best way to stay healthy is to exercise regularly
and eat a balanced diet. However, not everyone has the time or
motivation to do so. Ones who are looking for a quick and easy way to
improve their health might consider using wearable devices such as
fitness trackers or smartwatches. These devices can track your activity
levels, heart rate, and even monitor your sleep patterns. But be careful
when choosing which devices to use. Some cheaper options may not be as
accurate or reliable as others. It's important to do your research and
read reviews before making a purchase. Anyone have any advice on which
devices are the best for tracking fitness and health? Why I posted it in
technology - I think wearable devices are the future of health and
fitness. Not sure. But with all the advancements in technology, this
might be the place to uncover something new and exciting. Thanks.

### \['son', 'earth', 'know'\]

The earth is in danger, and it's all because of our actions. We have
exploited its resources and polluted its air and water. We must take
responsibility for our actions and work towards healing the earth. We
must also educate our children about the importance of taking care of
the planet, so that they can continue to do so in the future.

As a parent, it's important to teach your son about the world around
him. Teach him to appreciate the beauty of nature and to respect all
living things. Teach him about the importance of reducing waste and
conserving resources. By doing so, you will be setting him up for a
lifetime of responsible behavior.

It's also important to stay informed and educate yourself about the
world we live in. Knowledge is power, and the more you know, the better
equipped you will be to make informed decisions. Don't be afraid to
question the status quo and seek out alternative viewpoints.

We may never know everything there is to know about the world, but it's
important to keep learning and growing. The more we know, the better
equipped we will be to make positive changes and create a better future
for ourselves and future generations.

### \['flat', 'rna', '19'\]

Has anyone else received a message like this
<https://i.imgur.com/a5jHnHK.png>? It's a clear threat to my safety and
I'm concerned. How can this be allowed on Reddit?

On a different note, have you heard about the latest breakthrough in the
study of RNA? Scientists have discovered a new type of RNA that plays a
crucial role in gene regulation. This discovery could lead to new
treatments for diseases such as cancer and Alzheimer's.

Also, did you know that the number 19 is considered a lucky number in
many cultures? In Chinese culture, the number 19 is associated with
prosperity and good fortune. In Italian culture, the number 19 is
considered lucky because it is the number of the sun.

[^1]: To clarify that notion a bit, we can imagine any given vernacular
    linguistic event, a conversation in most instances, as a series of
    utterances exchanged among participants. Each utterance is nothing
    more, nor less, than a string of tokens most of which are words but
    some of which might be other kinds of sounds.

[^2]: Tools mentioned include Dragnet
    (https://dl.acm.org/doi/abs/10.1145/2487788.2487828) and Newspaper
    (https://github.com/codelucas/newspaper).

[^3]: This list of keywords was edited by hand to remove words that were
    either redundant, different forms of the same noun or verb, or
    struck me as not being very semantically rich.

[^4]: For those interested, the smaller set of words was: black, maybe,
    different, timeline, holes, galaxy, space, thought, hole, years,
    white, nazi, nazis, flag, just, supremacist, false, people,
    supremacy, like, debt, tax, ceiling, currency, pay, taxes, spending,
    default, money.
